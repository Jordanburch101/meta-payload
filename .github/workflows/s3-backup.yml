name: S3 to Dropbox Backup

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:  # Allow manual triggering

env:
  S3_ENDPOINT: 'https://vfmxvvugriytncpzypvm.supabase.co/storage/v1/s3'
  S3_REGION: 'ap-southeast-2'
  BACKUP_RETENTION_DAYS: 30
  BETTER_STACK_HEARTBEAT: 'https://uptime.betterstack.com/api/v1/heartbeat/PgcWvW8C3Navz8Eo3aYC1WF4'

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        pip install boto3 dropbox requests

    - name: Perform backup
      env:
        DROPBOX_APP_KEY: ${{ secrets.DROPBOX_APP_KEY }}
        DROPBOX_APP_SECRET: ${{ secrets.DROPBOX_APP_SECRET }}
        DROPBOX_REFRESH_TOKEN: ${{ secrets.DROPBOX_REFRESH_TOKEN }}
        S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
        S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
        S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
      run: |
        python - <<EOF
        import os
        import boto3
        import dropbox
        from dropbox import DropboxOAuth2FlowNoRedirect
        from datetime import datetime, timedelta
        import requests

        # S3 setup
        s3 = boto3.client('s3',
            endpoint_url=os.environ['S3_ENDPOINT'],
            aws_access_key_id=os.environ['S3_ACCESS_KEY_ID'],
            aws_secret_access_key=os.environ['S3_SECRET_ACCESS_KEY'],
            region_name=os.environ['S3_REGION']
        )

        # Dropbox setup with token refresh
        def get_dropbox_client():
            app_key = os.environ['DROPBOX_APP_KEY']
            app_secret = os.environ['DROPBOX_APP_SECRET']
            refresh_token = os.environ['DROPBOX_REFRESH_TOKEN']

            try:
                # Attempt to create a Dropbox client with the refresh token
                return dropbox.Dropbox(oauth2_refresh_token=refresh_token,
                                       app_key=app_key,
                                       app_secret=app_secret)
            except dropbox.exceptions.AuthError:
                print("Failed to authenticate. Attempting to refresh the token...")
                # If authentication fails, try to get a new access token
                auth_flow = DropboxOAuth2FlowNoRedirect(app_key, app_secret)
                oauth_result = auth_flow.refresh_access_token(refresh_token)
                
                # Use the new access token to create a Dropbox client
                return dropbox.Dropbox(oauth2_access_token=oauth_result.access_token,
                                       app_key=app_key,
                                       app_secret=app_secret)

        # Backup function
        def backup_s3_to_dropbox(dbx):
            bucket_name = os.environ['S3_BUCKET_NAME']
            today = datetime.now().strftime('%Y-%m-%d')
            backup_folder = f'/S3_Backups/{today}'

            # List all objects in the S3 bucket
            objects = s3.list_objects_v2(Bucket=bucket_name)

            for obj in objects.get('Contents', []):
                # Prepare local file path
                file_name = obj['Key']
                local_file_path = os.path.join('/tmp', file_name)

                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)

                # Download file from S3
                s3.download_file(bucket_name, file_name, local_file_path)

                # Upload file to Dropbox
                with open(local_file_path, 'rb') as f:
                    dbx.files_upload(f.read(), f'{backup_folder}/{file_name}')

                # Remove local file
                os.remove(local_file_path)

            print(f"Backup completed for {today}")

        # Remove old backups
        def remove_old_backups(dbx):
            retention_days = int(os.environ['BACKUP_RETENTION_DAYS'])
            cutoff_date = datetime.now() - timedelta(days=retention_days)

            result = dbx.files_list_folder('/S3_Backups')
            for entry in result.entries:
                if isinstance(entry, dropbox.files.FolderMetadata):
                    folder_date = datetime.strptime(entry.name, '%Y-%m-%d')
                    if folder_date < cutoff_date:
                        dbx.files_delete_v2(entry.path_display)
                        print(f"Removed old backup: {entry.name}")

        # Main execution
        try:
            dbx = get_dropbox_client()
            backup_s3_to_dropbox(dbx)
            remove_old_backups(dbx)
            # Send success heartbeat
            requests.get(os.environ['BETTER_STACK_HEARTBEAT'])
            print("Backup process completed successfully")
        except Exception as e:
            print(f"Error during backup process: {str(e)}")
            # Send failure heartbeat
            requests.get(os.environ['BETTER_STACK_HEARTBEAT'] + '/fail')
            raise e
        EOF

    - name: Check for errors
      if: failure()
      run: |
        echo "Backup process failed. Check the logs for more information."
